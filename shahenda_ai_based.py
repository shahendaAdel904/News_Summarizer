# -*- coding: utf-8 -*-
"""Shahenda_Ai_Based.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yg-VJb4mJaK79aIsBwYHvhO0dFHYnt9r
"""

!pip install langchain openai chromadb faiss-cpu newsapi-python

!pip install langchain-community

!pip install unstructured
!pip install "langchain>=0.0.242"

import os
import json
from newsapi import NewsApiClient
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains.summarize import load_summarize_chain
from langchain.llms import HuggingFaceHub
from langchain.text_splitter import RecursiveCharacterTextSplitter

import pandas as pd
import requests

#Shahenda Adel _ 21100796

news_key = "35ca292ff332401a8a9544ea624f0439"
groq_key = "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"

newsapi = NewsApiClient(api_key=news_key)

PREFERENCES_FILE = "user_preferences.json"
HISTORY_FILE = "search_history.json"
def load_json_file(filename, default_data):
    if os.path.exists(filename):
        with open(filename, "r") as file:
            return json.load(file)
    return default_data

def save_json_file(filename, data):
    with open(filename, "w") as file:
        json.dump(data, file)

def load_preferences():
    return load_json_file(PREFERENCES_FILE, {"topics": ["technology", "AI"]})

def save_preferences(preferences):
    save_json_file(PREFERENCES_FILE, preferences)

def load_history():
    return load_json_file(HISTORY_FILE, [])

def save_history(search_query):
    history = load_history()
    history.append(search_query)
    save_json_file(HISTORY_FILE, history)

def fetch_news_articles(topics):
    all_articles = []
    for topic in topics:
        if not isinstance(topic, list):
            topic = str(topic)

        articles = newsapi.get_everything(q=topic, language='en', sort_by='publishedAt', page_size=5)
        if "articles" in articles:
            article_contents = [article["content"] for article in articles["articles"] if article["content"]]
            all_articles.extend(article_contents)

        save_history(topic)  # Save search history
    return all_articles

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vectorstore = Chroma(embedding_function=embeddings, persist_directory="chroma_db")

def add_articles_to_vectorstore(articles):
    # Join all articles into a single string before splitting
    all_text = "".join(["".join(article) for article in articles])
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
    split_docs = text_splitter.split_text(all_text)  # Split the combined text
    vectorstore.add_texts(split_docs)

def summarize_articles(query, summary_type="brief"):
    retriever = vectorstore.as_retriever()
    relevant_docs = retriever.get_relevant_documents(query=query)

    if not relevant_docs:
        return "No relevant articles found for your query."
    if summary_type == "brief":
        llm = HuggingFaceHub(repo_id="facebook/bart-large-cnn", huggingfacehub_api_token="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")
        summarization_chain = load_summarize_chain(llm, chain_type="stuff")
    else:
        llm = HuggingFaceHub(repo_id="facebook/bart-large-cnn", huggingfacehub_api_token="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")
        summarization_chain = load_summarize_chain(llm, chain_type="refine")

    summary = summarization_chain.run(relevant_docs)
    return summary

def main():
    preferences = load_preferences()
    print("User preferences:", preferences["topics"])

    action = input("Choose action: [1] Fetch & Summarize News, [2] Update Preferences, [3] View Search History, [4] Search News on a Custom Topic: ")

    if action == "1":
        articles = fetch_news_articles(preferences["topics"])
        if not articles:
            print("No articles found for the selected topics.")
            return
        add_articles_to_vectorstore(articles)
        summary_type = input("Choose summary type: [brief/detailed]: ").strip().lower()
        summary = summarize_articles(summary_type)
        print("\nNews Summary:\n", summary)

    elif action == "2":
        new_topics = input("Enter new topics (comma-separated): ").split(",")
        preferences["topics"] = [t.strip() for t in new_topics]
        save_preferences(preferences)
        print("Preferences updated!")

    elif action == "3":
        history = load_history()
        print("\nSearch History:")
        for i, query in enumerate(history, 1):
            print(f"{i}. {query}")

    elif action == "4":  # NEW OPTION FOR CUSTOM SEARCH
          custom_topic = input("Enter a topic to search: ").strip()
          articles = fetch_news_articles([custom_topic])  # Fetch articles for the entered topic

          if not articles:
              print(f"No articles found for '{custom_topic}'.")
              return

          add_articles_to_vectorstore(articles)  # Store fetched articles in vectorstore

          summary_type = input("Choose summary type: [brief/detailed]: ").strip().lower()
          summary = summarize_articles(custom_topic, summary_type)  # Use custom topic as query
          print("\nNews Summary:\n", summary)


    else:
        print("Invalid option.")

if __name__ == "__main__":
    main()

